{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2c08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\arabe\\nlp-pikogpt-funkyai\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Set working directory to project root\n",
    "\n",
    "import os\n",
    "\n",
    "# Change to project root (one level up from notebooks/)\n",
    "os.chdir(\"..\")\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2a9a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imports...\n",
      "✓ datasets library works\n",
      "✓ All imports successful\n",
      "\n",
      "Ready to start EDA!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Test Setup\n",
    "print(\"Testing imports...\")\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "print(\"✓ datasets library works\")\n",
    "\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(\"\\nReady to start EDA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776e51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenWebText from HuggingFace...\n",
      "Using streaming to avoid downloading full 60GB dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413b6d02f5564ebd968252f734bde601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cf3af34ab248449198d73cf0b1885d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 10,000 samples for EDA...\n",
      "  Collected 0 / 10,000 samples...\n",
      "  Collected 2,000 / 10,000 samples...\n",
      "  Collected 4,000 / 10,000 samples...\n",
      "  Collected 6,000 / 10,000 samples...\n",
      "  Collected 8,000 / 10,000 samples...\n",
      "\n",
      "✓ Loaded successfully!\n",
      "Subset size: 10,000 documents\n",
      "\n",
      "Example document (first 300 chars):\n",
      "Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\n",
      "\n",
      "The decision left CNN Chief Medical Corresponden\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load OpenWebText from HuggingFace (Optimized)\n",
    "\n",
    "print(\"Loading OpenWebText from HuggingFace...\")\n",
    "print(\"Using streaming to avoid downloading full 60GB dataset\\n\")\n",
    "\n",
    "dataset_stream = load_dataset(\n",
    "    \"Skylion007/openwebtext\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"Collecting 10,000 samples for EDA...\")\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "samples = []\n",
    "for i, sample in enumerate(dataset_stream):\n",
    "    if i >= SAMPLE_SIZE:\n",
    "        break\n",
    "    samples.append(sample)\n",
    "    if i % 2000 == 0:\n",
    "        print(f\"  Collected {i:,} / {SAMPLE_SIZE:,} samples...\")\n",
    "\n",
    "subset = Dataset.from_list(samples)\n",
    "\n",
    "print(f\"\\n✓ Loaded successfully!\")\n",
    "print(f\"Subset size: {len(subset):,} documents\")\n",
    "print(f\"\\nExample document (first 300 chars):\")\n",
    "print(subset[0]['text'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6de7ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP26 test split from disk...\n",
      "✓ Test dataset loaded!\n",
      "Test dataset size: 400,689 documents\n",
      "\n",
      "Sample test document (first 300 chars):\n",
      "Image caption A fragment of an iron vessel was found during the excavation\n",
      "\n",
      "Archaeologists say they have confirmed the location of a meeting place of a medieval Norse parliament.\n",
      "\n",
      "Called a \"thing\", evidence of the mound was uncovered during excavations of Dingwall's Cromartie Memorial car park.\n",
      "\n",
      "Whe\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load the NLP26 Test Split\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(\"Loading NLP26 test split from disk...\")\n",
    "\n",
    "TEST_DATA_PATH = \"data/raw/NLP26_OWT_eval/test\"\n",
    "\n",
    "try:\n",
    "    test_dataset = load_from_disk(TEST_DATA_PATH)\n",
    "    print(f\"✓ Test dataset loaded!\")\n",
    "    print(f\"Test dataset size: {len(test_dataset):,} documents\")\n",
    "    print(f\"\\nSample test document (first 300 chars):\")\n",
    "    print(test_dataset[0]['text'][:300])\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading test data: {e}\")\n",
    "    print(f\"\\nMake sure you downloaded the test folder to: {TEST_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8695680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING DATA (OpenWebText subset)\n",
      "============================================================\n",
      "Number of samples: 10,000\n",
      "Features: {'text': Value('string')}\n",
      "\n",
      "Example document:\n",
      "Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\n",
      "\n",
      "The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\n",
      "\n",
      "CNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the B...\n",
      "\n",
      "============================================================\n",
      "TEST DATA (NLP26 split - DO NOT TRAIN ON THIS)\n",
      "============================================================\n",
      "Number of samples: 400,689\n",
      "Features: {'text': Value('string')}\n",
      "\n",
      "Example document:\n",
      "Image caption A fragment of an iron vessel was found during the excavation\n",
      "\n",
      "Archaeologists say they have confirmed the location of a meeting place of a medieval Norse parliament.\n",
      "\n",
      "Called a \"thing\", evidence of the mound was uncovered during excavations of Dingwall's Cromartie Memorial car park.\n",
      "\n",
      "When it was constructed in the 11th Century, the thing would have been on a man-made islet in the estuary of the Peffery.\n",
      "\n",
      "Archaeologists and historians believe it was built on the instructions of Thorfi...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Compare training and test data structure\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING DATA (OpenWebText subset)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {len(subset):,}\")\n",
    "print(f\"Features: {subset.features}\")\n",
    "print(f\"\\nExample document:\\n{subset[0]['text'][:500]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST DATA (NLP26 split - DO NOT TRAIN ON THIS)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples: {len(test_dataset):,}\")\n",
    "print(f\"Features: {test_dataset.features}\")\n",
    "print(f\"\\nExample document:\\n{test_dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713d2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing document lengths...\n",
      "\n",
      "Document Length Statistics:\n",
      "  Min:    658 chars\n",
      "  Max:    100,000 chars\n",
      "  Mean:   4,859 chars\n",
      "  Median: 3,169 chars\n",
      "\n",
      "Length Distribution:\n",
      "       0-100:     0 (  0.0%)\n",
      "     100-500:     0 (  0.0%)\n",
      "      500-1K:   580 (  5.8%)\n",
      "       1K-5K:  6553 ( 65.5%)\n",
      "      5K-10K:  2006 ( 20.1%)\n",
      "     10K-50K:   828 (  8.3%)\n",
      "        50K+:    33 (  0.3%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Document Length Analysis\n",
    "\n",
    "print(\"Analyzing document lengths...\\n\")\n",
    "\n",
    "lengths = [len(doc['text']) for doc in subset]\n",
    "\n",
    "print(\"Document Length Statistics:\")\n",
    "print(f\"  Min:    {min(lengths):,} chars\")\n",
    "print(f\"  Max:    {max(lengths):,} chars\")\n",
    "print(f\"  Mean:   {sum(lengths)/len(lengths):,.0f} chars\")\n",
    "print(f\"  Median: {sorted(lengths)[len(lengths)//2]:,} chars\")\n",
    "\n",
    "# Distribution buckets\n",
    "print(\"\\nLength Distribution:\")\n",
    "buckets = [0, 100, 500, 1000, 5000, 10000, 50000, float('inf')]\n",
    "bucket_names = ['0-100', '100-500', '500-1K', '1K-5K', '5K-10K', '10K-50K', '50K+']\n",
    "\n",
    "for i in range(len(buckets)-1):\n",
    "    count = sum(1 for l in lengths if buckets[i] <= l < buckets[i+1])\n",
    "    pct = count / len(lengths) * 100\n",
    "    print(f\"  {bucket_names[i]:>10}: {count:>5} ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28663435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting languages (this takes 1-2 minutes)...\n",
      "\n",
      "  Processed 250/1000...\n",
      "  Processed 500/1000...\n",
      "  Processed 750/1000...\n",
      "  Processed 1000/1000...\n",
      "\n",
      "Language Distribution:\n",
      "          en:   997 ( 99.7%)\n",
      "          nl:     1 (  0.1%)\n",
      "          es:     1 (  0.1%)\n",
      "          de:     1 (  0.1%)\n",
      "\n",
      "Non-English content: 3/1000 (0.3%)\n",
      "^ This content needs to be filtered out\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Language Detection\n",
    "\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "def detect_language_safe(text):\n",
    "    \"\"\"Detect language with error handling.\"\"\"\n",
    "    try:\n",
    "        return detect(text[:1000])  # Use first 1000 chars for speed\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "print(\"Detecting languages (this takes 1-2 minutes)...\\n\")\n",
    "\n",
    "# Check a sample of documents\n",
    "LANG_SAMPLE_SIZE = 1000\n",
    "languages = []\n",
    "\n",
    "for i in range(LANG_SAMPLE_SIZE):\n",
    "    lang = detect_language_safe(subset[i]['text'])\n",
    "    languages.append(lang)\n",
    "    if (i + 1) % 250 == 0:\n",
    "        print(f\"  Processed {i + 1}/{LANG_SAMPLE_SIZE}...\")\n",
    "\n",
    "# Count results\n",
    "from collections import Counter\n",
    "lang_counts = Counter(languages)\n",
    "\n",
    "print(\"\\nLanguage Distribution:\")\n",
    "for lang, count in lang_counts.most_common(10):\n",
    "    pct = count / len(languages) * 100\n",
    "    print(f\"  {lang:>10}: {count:>5} ({pct:>5.1f}%)\")\n",
    "\n",
    "non_english = sum(c for l, c in lang_counts.items() if l != 'en')\n",
    "print(f\"\\nNon-English content: {non_english}/{len(languages)} ({non_english/len(languages)*100:.1f}%)\")\n",
    "print(\"^ This content needs to be filtered out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b191495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for HTML content...\n",
      "\n",
      "Documents containing HTML: 131/10000 (1.3%)\n",
      "\n",
      "Most common HTML tags:\n",
      "  <span>: 111\n",
      "  <div>: 63\n",
      "  <br>: 60\n",
      "  <string>: 57\n",
      "  <T>: 44\n",
      "  <a>: 44\n",
      "  <TestObject>: 37\n",
      "  <char>: 26\n",
      "  <key>: 22\n",
      "  <typename>: 14\n",
      "\n",
      "============================================================\n",
      "EXAMPLE DOCUMENT WITH HTML:\n",
      "============================================================\n",
      "Over the years, I've learned to be cautious with C++ pointers. In particular, I'm always very careful about who owns a given pointer, and who's in charge of calling delete on it. But my caution often forces me to write deliberately inefficient functions. For example:\n",
      "\n",
      "vector < string > tokenize_string ( const string & text );\n",
      "\n",
      "Here, we have a large string text , and we want to split it into a vector of tokens. This function is nice and safe, but it allocates one string for every token in the input. Now, if we were feeling reckless, we could avoid these allocations by returning a vector of poin\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: HTML Detection\n",
    "\n",
    "import re\n",
    "\n",
    "def has_html(text):\n",
    "    \"\"\"Check if text contains HTML tags.\"\"\"\n",
    "    return bool(re.search(r'<[^>]+>', text))\n",
    "\n",
    "def find_html_tags(text):\n",
    "    \"\"\"Find all HTML tags in text.\"\"\"\n",
    "    pattern = r'<([a-zA-Z][a-zA-Z0-9]*)\\b[^>]*>'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "print(\"Checking for HTML content...\\n\")\n",
    "\n",
    "html_count = 0\n",
    "all_tags = []\n",
    "\n",
    "for doc in subset:\n",
    "    text = doc['text']\n",
    "    if has_html(text):\n",
    "        html_count += 1\n",
    "        tags = find_html_tags(text)\n",
    "        all_tags.extend(tags)\n",
    "\n",
    "print(f\"Documents containing HTML: {html_count}/{len(subset)} ({html_count/len(subset)*100:.1f}%)\")\n",
    "\n",
    "if all_tags:\n",
    "    print(\"\\nMost common HTML tags:\")\n",
    "    tag_counts = Counter(all_tags)\n",
    "    for tag, count in tag_counts.most_common(10):\n",
    "        print(f\"  <{tag}>: {count}\")\n",
    "\n",
    "# Show example if found\n",
    "if html_count > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE DOCUMENT WITH HTML:\")\n",
    "    print(\"=\" * 60)\n",
    "    for doc in subset:\n",
    "        if has_html(doc['text']):\n",
    "            print(doc['text'][:600])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e5dec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for URLs and code...\n",
      "\n",
      "Documents with URLs:  724/10000 (7.2%)\n",
      "Documents with code:  113/10000 (1.1%)\n",
      "Total URLs found:     1707\n",
      "\n",
      "============================================================\n",
      "EXAMPLE DOCUMENT WITH CODE:\n",
      "============================================================\n",
      "NOTE: this website was build using Miraj. The source code is available at: miraj-project/homepage . Many other simple examples with commented code are available at miraj-project/demos/hello-world\n",
      "\n",
      "Components can also be easily defined as one-off elements for use in a single page. Both page and components can be defined in the same project.\n",
      "\n",
      "Miraj also makes it very easy to define and share component libraries. Multiple components may be defined across multiple namespaces; a deflibrary macro then assembles any combination of components into a library namespace, which is independent of the defining namespaces. Miraj can automatically generate a demo page for previewing/testing components under development.\n",
      "\n",
      "Things get a little more complicated when you add web components. Miraj allows the pr\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: URL and Code Detection\n",
    "\n",
    "import re\n",
    "\n",
    "def count_urls(text):\n",
    "    \"\"\"Count URLs in text.\"\"\"\n",
    "    pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "def contains_code(text):\n",
    "    \"\"\"Detect if text contains code.\"\"\"\n",
    "    code_patterns = [\n",
    "        r'```',                          # Markdown code blocks\n",
    "        r'def \\w+\\s*\\(',                 # Python function\n",
    "        r'function\\s+\\w+\\s*\\(',          # JavaScript function\n",
    "        r'class\\s+\\w+\\s*[:\\{]',          # Class definition\n",
    "        r'import\\s+[\\w.]+',              # Import statement\n",
    "        r'#include\\s*<',                 # C/C++ include\n",
    "    ]\n",
    "    for pattern in code_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"Checking for URLs and code...\\n\")\n",
    "\n",
    "docs_with_urls = 0\n",
    "docs_with_code = 0\n",
    "total_urls = 0\n",
    "\n",
    "for doc in subset:\n",
    "    text = doc['text']\n",
    "    urls = count_urls(text)\n",
    "    if urls > 0:\n",
    "        docs_with_urls += 1\n",
    "        total_urls += urls\n",
    "    if contains_code(text):\n",
    "        docs_with_code += 1\n",
    "\n",
    "print(f\"Documents with URLs:  {docs_with_urls}/{len(subset)} ({docs_with_urls/len(subset)*100:.1f}%)\")\n",
    "print(f\"Documents with code:  {docs_with_code}/{len(subset)} ({docs_with_code/len(subset)*100:.1f}%)\")\n",
    "print(f\"Total URLs found:     {total_urls}\")\n",
    "\n",
    "# Show example with code if found\n",
    "if docs_with_code > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE DOCUMENT WITH CODE:\")\n",
    "    print(\"=\" * 60)\n",
    "    for doc in subset:\n",
    "        if contains_code(doc['text']):\n",
    "            print(doc['text'][:800])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0cf258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for quality issues...\n",
      "\n",
      "Documents with issues: 100/10000 (1.0%)\n",
      "\n",
      "Issue breakdown:\n",
      "  repeated_chars: 85\n",
      "  replacement_char: 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Special Characters and Quality Issues\n",
    "\n",
    "import re\n",
    "\n",
    "def find_issues(text):\n",
    "    \"\"\"Find various text quality issues.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if '�' in text:\n",
    "        issues.append('replacement_char')\n",
    "    if re.search(r'(.)\\1{15,}', text):\n",
    "        issues.append('repeated_chars')\n",
    "    if re.search(r'\\n{5,}', text):\n",
    "        issues.append('excessive_newlines')\n",
    "    \n",
    "    return issues\n",
    "\n",
    "print(\"Checking for quality issues...\\n\")\n",
    "\n",
    "issue_counts = Counter()\n",
    "problematic_docs = 0\n",
    "\n",
    "for doc in subset:\n",
    "    issues = find_issues(doc['text'])\n",
    "    if issues:\n",
    "        problematic_docs += 1\n",
    "        issue_counts.update(issues)\n",
    "\n",
    "print(f\"Documents with issues: {problematic_docs}/{len(subset)} ({problematic_docs/len(subset)*100:.1f}%)\")\n",
    "\n",
    "if issue_counts:\n",
    "    print(\"\\nIssue breakdown:\")\n",
    "    for issue, count in issue_counts.most_common():\n",
    "        print(f\"  {issue}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf545b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           EDA FINDINGS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "DATASET OVERVIEW\n",
      "----------------\n",
      "- Training data (OpenWebText): ~8 million documents total, using 10K sample for EDA\n",
      "- Test data (NLP26 split): 400,689 documents - MUST BE EXCLUDED FROM TRAINING\n",
      "\n",
      "DOCUMENT LENGTH\n",
      "---------------\n",
      "- Min: 658 chars | Max: 100,000 chars | Median: 3,169 chars\n",
      "- Most documents (65.5%) are between 1K-5K characters\n",
      "- No documents under 500 characters in this sample\n",
      "\n",
      "LANGUAGE DISTRIBUTION\n",
      "---------------------\n",
      "- English: 99.7%\n",
      "- Non-English: 0.3% (Dutch, Spanish, German detected)\n",
      "- Action: Filter non-English content using langdetect\n",
      "\n",
      "HTML CONTENT\n",
      "------------\n",
      "- Documents with HTML: 1.3%\n",
      "- Common tags: <span>, <div>, <br>, <a>\n",
      "- Action: Remove HTML tags with regex\n",
      "\n",
      "URLs\n",
      "----\n",
      "- Documents with URLs: 7.2%\n",
      "- Total URLs found: 1,707 in 10K sample\n",
      "- Action: Remove URLs with regex\n",
      "\n",
      "CODE SNIPPETS\n",
      "-------------\n",
      "- Documents with code: 1.1%\n",
      "- Types: Python, JavaScript, XML/HTML code examples\n",
      "- Action: Remove markdown code blocks, consider filtering heavy-code docs\n",
      "\n",
      "QUALITY ISSUES\n",
      "--------------\n",
      "- Documents with issues: 1.0%\n",
      "- Repeated characters: 85 cases\n",
      "- Replacement characters (�): 15 cases\n",
      "- Action: Clean or filter problematic documents\n",
      "\n",
      "================================================================================\n",
      "                         PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Based on these findings, the preprocessing script should:\n",
      "\n",
      "1. LANGUAGE FILTER    → Keep only English (langdetect == 'en')\n",
      "2. TEST SET FILTER    → Remove sentences appearing in NLP26 test split (CRITICAL!)\n",
      "3. HTML REMOVAL       → Strip all HTML tags\n",
      "4. URL REMOVAL        → Remove http://, https://, www. links\n",
      "5. CODE REMOVAL       → Remove markdown code blocks (```)\n",
      "6. SPECIAL CHARS      → Remove replacement characters (�), normalize whitespace\n",
      "7. QUALITY FILTER     → Remove docs with excessive repetition\n",
      "8. LENGTH FILTER      → Consider minimum length threshold (e.g., 100 chars after cleaning)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: EDA Summary\n",
    "\n",
    "summary = \"\"\"\n",
    "================================================================================\n",
    "                           EDA FINDINGS SUMMARY\n",
    "================================================================================\n",
    "\n",
    "DATASET OVERVIEW\n",
    "----------------\n",
    "- Training data (OpenWebText): ~8 million documents total, using 10K sample for EDA\n",
    "- Test data (NLP26 split): 400,689 documents - MUST BE EXCLUDED FROM TRAINING\n",
    "\n",
    "DOCUMENT LENGTH\n",
    "---------------\n",
    "- Min: 658 chars | Max: 100,000 chars | Median: 3,169 chars\n",
    "- Most documents (65.5%) are between 1K-5K characters\n",
    "- No documents under 500 characters in this sample\n",
    "\n",
    "LANGUAGE DISTRIBUTION\n",
    "---------------------\n",
    "- English: 99.7%\n",
    "- Non-English: 0.3% (Dutch, Spanish, German detected)\n",
    "- Action: Filter non-English content using langdetect\n",
    "\n",
    "HTML CONTENT\n",
    "------------\n",
    "- Documents with HTML: 1.3%\n",
    "- Common tags: <span>, <div>, <br>, <a>\n",
    "- Action: Remove HTML tags with regex\n",
    "\n",
    "URLs\n",
    "----\n",
    "- Documents with URLs: 7.2%\n",
    "- Total URLs found: 1,707 in 10K sample\n",
    "- Action: Remove URLs with regex\n",
    "\n",
    "CODE SNIPPETS\n",
    "-------------\n",
    "- Documents with code: 1.1%\n",
    "- Types: Python, JavaScript, XML/HTML code examples\n",
    "- Action: Remove markdown code blocks, consider filtering heavy-code docs\n",
    "\n",
    "QUALITY ISSUES\n",
    "--------------\n",
    "- Documents with issues: 1.0%\n",
    "- Repeated characters: 85 cases\n",
    "- Replacement characters (�): 15 cases\n",
    "- Action: Clean or filter problematic documents\n",
    "\n",
    "================================================================================\n",
    "                         PREPROCESSING PIPELINE\n",
    "================================================================================\n",
    "\n",
    "Based on these findings, the preprocessing script should:\n",
    "\n",
    "1. LANGUAGE FILTER    → Keep only English (langdetect == 'en')\n",
    "2. TEST SET FILTER    → Remove sentences appearing in NLP26 test split (CRITICAL!)\n",
    "3. HTML REMOVAL       → Strip all HTML tags\n",
    "4. URL REMOVAL        → Remove http://, https://, www. links\n",
    "5. CODE REMOVAL       → Remove markdown code blocks (```)\n",
    "6. SPECIAL CHARS      → Remove replacement characters (�), normalize whitespace\n",
    "7. QUALITY FILTER     → Remove docs with excessive repetition\n",
    "8. LENGTH FILTER      → Consider minimum length threshold (e.g., 100 chars after cleaning)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-pikogpt-funkyai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
